{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaf225de-a6c3-4c50-9f3b-3c99bc6b9059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Transformer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pickle\n",
    "\n",
    "## NanoGPS adds\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "587773bc-dcfe-4766-9c4a-e84a691f99fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n",
      "device cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA Available: \",torch.cuda.is_available())\n",
    "use_cuda=True\n",
    "device = torch.device(\"cuda:0\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e247f784-e495-400e-bf94-998e97e899aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig pairs for the experiment ['AB', 'AC', 'AD', 'AE', 'AF', 'AG']\n",
      "Creating experiment with 7 members, 4 classes and 21 dummy stimulus.\n",
      "Experiment trials created!\n"
     ]
    }
   ],
   "source": [
    "%run create_trials.py \"AB\",\"AC\",\"AD\",\"AE\",\"AF\",\"AG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9c9c376-5034-4f2b-a3ab-5da53222327d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_subset</th>\n",
       "      <th>st_sample</th>\n",
       "      <th>st_comp1</th>\n",
       "      <th>st_comp2</th>\n",
       "      <th>st_comp3</th>\n",
       "      <th>option_answer</th>\n",
       "      <th>st_comparison</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18466</th>\n",
       "      <td>train</td>\n",
       "      <td>A3</td>\n",
       "      <td>F2</td>\n",
       "      <td>E3</td>\n",
       "      <td>E1</td>\n",
       "      <td>O_2</td>\n",
       "      <td>E3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10263</th>\n",
       "      <td>train</td>\n",
       "      <td>A1</td>\n",
       "      <td>D1</td>\n",
       "      <td>D2</td>\n",
       "      <td>E3</td>\n",
       "      <td>O_1</td>\n",
       "      <td>D1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17470</th>\n",
       "      <td>train</td>\n",
       "      <td>A2</td>\n",
       "      <td>G1</td>\n",
       "      <td>E2</td>\n",
       "      <td>D4</td>\n",
       "      <td>O_2</td>\n",
       "      <td>E2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24357</th>\n",
       "      <td>train</td>\n",
       "      <td>A4</td>\n",
       "      <td>F4</td>\n",
       "      <td>B3</td>\n",
       "      <td>G1</td>\n",
       "      <td>O_1</td>\n",
       "      <td>F4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7880</th>\n",
       "      <td>train</td>\n",
       "      <td>A3</td>\n",
       "      <td>D4</td>\n",
       "      <td>B1</td>\n",
       "      <td>C3</td>\n",
       "      <td>O_3</td>\n",
       "      <td>C3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3547</th>\n",
       "      <td>train</td>\n",
       "      <td>A3</td>\n",
       "      <td>F1</td>\n",
       "      <td>B3</td>\n",
       "      <td>A2</td>\n",
       "      <td>O_2</td>\n",
       "      <td>B3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16554</th>\n",
       "      <td>train</td>\n",
       "      <td>A2</td>\n",
       "      <td>E2</td>\n",
       "      <td>C1</td>\n",
       "      <td>F4</td>\n",
       "      <td>O_1</td>\n",
       "      <td>E2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16619</th>\n",
       "      <td>train</td>\n",
       "      <td>A2</td>\n",
       "      <td>D4</td>\n",
       "      <td>C4</td>\n",
       "      <td>E2</td>\n",
       "      <td>O_3</td>\n",
       "      <td>E2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8101</th>\n",
       "      <td>train</td>\n",
       "      <td>A3</td>\n",
       "      <td>D1</td>\n",
       "      <td>C3</td>\n",
       "      <td>A4</td>\n",
       "      <td>O_2</td>\n",
       "      <td>C3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19888</th>\n",
       "      <td>train</td>\n",
       "      <td>A4</td>\n",
       "      <td>G2</td>\n",
       "      <td>E4</td>\n",
       "      <td>A3</td>\n",
       "      <td>O_2</td>\n",
       "      <td>E4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8653</th>\n",
       "      <td>train</td>\n",
       "      <td>A3</td>\n",
       "      <td>G1</td>\n",
       "      <td>C3</td>\n",
       "      <td>B2</td>\n",
       "      <td>O_2</td>\n",
       "      <td>C3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4904</th>\n",
       "      <td>train</td>\n",
       "      <td>A4</td>\n",
       "      <td>F1</td>\n",
       "      <td>F3</td>\n",
       "      <td>B4</td>\n",
       "      <td>O_3</td>\n",
       "      <td>B4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28198</th>\n",
       "      <td>train</td>\n",
       "      <td>A3</td>\n",
       "      <td>B1</td>\n",
       "      <td>G3</td>\n",
       "      <td>C4</td>\n",
       "      <td>O_2</td>\n",
       "      <td>G3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26036</th>\n",
       "      <td>train</td>\n",
       "      <td>A1</td>\n",
       "      <td>F2</td>\n",
       "      <td>F4</td>\n",
       "      <td>G1</td>\n",
       "      <td>O_3</td>\n",
       "      <td>G1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25743</th>\n",
       "      <td>train</td>\n",
       "      <td>A1</td>\n",
       "      <td>G1</td>\n",
       "      <td>E4</td>\n",
       "      <td>E3</td>\n",
       "      <td>O_1</td>\n",
       "      <td>G1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6484</th>\n",
       "      <td>train</td>\n",
       "      <td>A2</td>\n",
       "      <td>D4</td>\n",
       "      <td>C2</td>\n",
       "      <td>E3</td>\n",
       "      <td>O_2</td>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3267</th>\n",
       "      <td>train</td>\n",
       "      <td>A3</td>\n",
       "      <td>B3</td>\n",
       "      <td>E2</td>\n",
       "      <td>D1</td>\n",
       "      <td>O_1</td>\n",
       "      <td>B3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2485</th>\n",
       "      <td>train</td>\n",
       "      <td>A2</td>\n",
       "      <td>C4</td>\n",
       "      <td>B2</td>\n",
       "      <td>D1</td>\n",
       "      <td>O_2</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14894</th>\n",
       "      <td>train</td>\n",
       "      <td>A4</td>\n",
       "      <td>E1</td>\n",
       "      <td>B2</td>\n",
       "      <td>D4</td>\n",
       "      <td>O_3</td>\n",
       "      <td>D4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9175</th>\n",
       "      <td>train</td>\n",
       "      <td>A4</td>\n",
       "      <td>B1</td>\n",
       "      <td>C4</td>\n",
       "      <td>C3</td>\n",
       "      <td>O_2</td>\n",
       "      <td>C4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample_subset st_sample st_comp1 st_comp2 st_comp3 option_answer  \\\n",
       "18466         train        A3       F2       E3       E1           O_2   \n",
       "10263         train        A1       D1       D2       E3           O_1   \n",
       "17470         train        A2       G1       E2       D4           O_2   \n",
       "24357         train        A4       F4       B3       G1           O_1   \n",
       "7880          train        A3       D4       B1       C3           O_3   \n",
       "3547          train        A3       F1       B3       A2           O_2   \n",
       "16554         train        A2       E2       C1       F4           O_1   \n",
       "16619         train        A2       D4       C4       E2           O_3   \n",
       "8101          train        A3       D1       C3       A4           O_2   \n",
       "19888         train        A4       G2       E4       A3           O_2   \n",
       "8653          train        A3       G1       C3       B2           O_2   \n",
       "4904          train        A4       F1       F3       B4           O_3   \n",
       "28198         train        A3       B1       G3       C4           O_2   \n",
       "26036         train        A1       F2       F4       G1           O_3   \n",
       "25743         train        A1       G1       E4       E3           O_1   \n",
       "6484          train        A2       D4       C2       E3           O_2   \n",
       "3267          train        A3       B3       E2       D1           O_1   \n",
       "2485          train        A2       C4       B2       D1           O_2   \n",
       "14894         train        A4       E1       B2       D4           O_3   \n",
       "9175          train        A4       B1       C4       C3           O_2   \n",
       "\n",
       "      st_comparison  \n",
       "18466            E3  \n",
       "10263            D1  \n",
       "17470            E2  \n",
       "24357            F4  \n",
       "7880             C3  \n",
       "3547             B3  \n",
       "16554            E2  \n",
       "16619            E2  \n",
       "8101             C3  \n",
       "19888            E4  \n",
       "8653             C3  \n",
       "4904             B4  \n",
       "28198            G3  \n",
       "26036            G1  \n",
       "25743            G1  \n",
       "6484             C2  \n",
       "3267             B3  \n",
       "2485             B2  \n",
       "14894            D4  \n",
       "9175             C4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_info.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e328d1f8-436f-42e7-bf73-e584acf9f3ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trials_info_columns=[\"st_sample\",\"st_comp1\",\"st_comp2\",\"st_comp3\",\"option_answer\"]#\"st_comparison\"\n",
    "\n",
    "train_trials_corpus_df=train_info[trials_info_columns]\n",
    "train_dummy_trials_corpus_df=train_dummy_info[trials_info_columns]#\"st_comparison\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28a3350e-ac19-4fd6-b4ec-9843fe0a7fa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokens_list=list(stims_random.keys())\n",
    "tokens_list=sorted(list(set((list(np.array(train_trials_corpus_df).reshape([1,-1])[0])+(list(np.array(train_dummy_trials_corpus_df).reshape([1,-1])[0]))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e007479-c50e-4012-98d1-95a2f280210b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_to_index = {token: index for index, token in enumerate(tokens_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48ecc37c-b7fa-46a7-bf8b-691b676688d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode corpus in positional tokens train data class member comparison\n",
    "encoded_training_data = np.array([[token_to_index[token] for token in sequence] for sequence in np.array(train_trials_corpus_df)])\n",
    "train_tensor_encoded=torch.from_numpy(encoded_training_data).int().to(device)\n",
    "train_tensor_encoded=train_tensor_encoded.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80319013-bb32-4b78-9cfe-34974ad0621a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encode corpus in positional tokens Train data Dummy comparison\n",
    "encoded_training_data_dummy = np.array([[token_to_index[token] for token in sequence] for sequence in np.array(train_dummy_trials_corpus_df)])\n",
    "train_dummy_tensor_encoded=torch.from_numpy(encoded_training_data_dummy).int().to(device)\n",
    "train_dummy_tensor_encoded=train_dummy_tensor_encoded.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c441e51-6f27-4146-a754-7b43ade70d37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set seed for reproductibility\n",
    "torch.manual_seed(183)\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 4 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 200\n",
    "\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "341f5a0c-0e84-438a-916c-d45b791125db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = tokens_list\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: [itos[i] for i in l]#lambda l: ','.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "122c6bbc-01a3-4df5-83c3-3761438b4b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(data):\n",
    "    # data must be train trials encoded and transformed in torch tensor\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    # data = train_tensor_encoded #if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data), (batch_size,)) \n",
    "    x = torch.stack([data[i,:block_size] for i in ix])\n",
    "    y = torch.stack([data[i,1:] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    # data = train_tensor_encoded# Modify for train several models\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        X, Y = get_batch(data)\n",
    "        logits, loss = model(X, Y)\n",
    "        losses[k] = loss.item()\n",
    "    out[\"train\"] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882d7d9b-dc38-4907-8686-d42a02ef790e",
   "metadata": {},
   "source": [
    "The next cell defines if the model is a decoder or encoder block.\n",
    "\n",
    "line 21 `wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))` adds the mask filter in the GPT decoder block\n",
    "\n",
    "remove this mask filter and you get the Encoder (BERT) block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "584160b4-7b06-4dde-9016-b77fdd99918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        # wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T) #filter communication with past# comment this and you have an encoder block\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "283a6d2d-132e-47e5-b483-60e498be6981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I created this function for call the trained model \n",
    "\n",
    "def get_trials_answers(tensor_encoded, model_trained, tell_me=True, tell_me_n_trials=1000):\n",
    "    # tensor_encoded: Pytorch tensor long(float64) type [sample, comp_1, comp_2, comp_3, comp_response] of all group trials\n",
    "    max_iters=tensor_encoded.shape[0]\n",
    "    response_stimulus=[]\n",
    "    for count, trial in enumerate(tensor_encoded[:,:-1]):\n",
    "        tokens_response=model_trained.generate(trial.reshape([1,-1]), max_new_tokens=1)[0].tolist()\n",
    "        response=decode(tokens_response)[-1]\n",
    "        response_stimulus.append(response)\n",
    "\n",
    "        if tell_me:\n",
    "            # tell me the progress every tell_me_n_trials\n",
    "            if count % tell_me_n_trials == 0 or count == max_iters - 1:\n",
    "                print(f\"{count} ({100.0*(count/max_iters):.2f}%) trials processed\")\n",
    "    return response_stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e3ec339-32f2-4fe7-8a78-663b90fff760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########### Model_1 train trials class comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48a1e775-cc9a-4fcb-bf30-184bbdec56bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.682164 M parameters\n",
      "step 0: train loss 4.0431\n",
      "step 500: train loss 0.0015\n",
      "step 1000: train loss 0.0004\n",
      "step 1500: train loss 0.0001\n",
      "step 2000: train loss 0.0001\n",
      "step 2500: train loss 0.0006\n",
      "step 3000: train loss 0.0001\n",
      "step 3500: train loss 0.0001\n",
      "step 4000: train loss 0.0002\n",
      "step 4500: train loss 0.0002\n",
      "step 4999: train loss 0.0000\n"
     ]
    }
   ],
   "source": [
    "model_class = GPTLanguageModel()\n",
    "m_class = model_class.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m_class.parameters())/1e6, 'M parameters')\n",
    "\n",
    "data=train_tensor_encoded\n",
    "model=m_class\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model_class.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        # print(losses)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(data)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model_class(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44a0302b-9e35-4d48-8a04-49c60308f2e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (0.00%) trials processed\n",
      "1000 (3.31%) trials processed\n",
      "2000 (6.61%) trials processed\n",
      "3000 (9.92%) trials processed\n",
      "4000 (13.23%) trials processed\n",
      "5000 (16.53%) trials processed\n",
      "6000 (19.84%) trials processed\n",
      "7000 (23.15%) trials processed\n",
      "8000 (26.46%) trials processed\n",
      "9000 (29.76%) trials processed\n",
      "10000 (33.07%) trials processed\n",
      "11000 (36.38%) trials processed\n",
      "12000 (39.68%) trials processed\n",
      "13000 (42.99%) trials processed\n",
      "14000 (46.30%) trials processed\n",
      "15000 (49.60%) trials processed\n",
      "16000 (52.91%) trials processed\n",
      "17000 (56.22%) trials processed\n",
      "18000 (59.52%) trials processed\n",
      "19000 (62.83%) trials processed\n",
      "20000 (66.14%) trials processed\n",
      "21000 (69.44%) trials processed\n",
      "22000 (72.75%) trials processed\n",
      "23000 (76.06%) trials processed\n",
      "24000 (79.37%) trials processed\n",
      "25000 (82.67%) trials processed\n",
      "26000 (85.98%) trials processed\n",
      "27000 (89.29%) trials processed\n",
      "28000 (92.59%) trials processed\n",
      "29000 (95.90%) trials processed\n",
      "30000 (99.21%) trials processed\n",
      "30239 (100.00%) trials processed\n"
     ]
    }
   ],
   "source": [
    "#### Predict model data\n",
    "train_responses_class=get_trials_answers(train_tensor_encoded, m_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f0fcae8-46ad-4c1b-a938-7d491438718f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########### Model_2 train trials dummy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3851ad4-7c59-413d-ac81-754b46fb029a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.682164 M parameters\n",
      "step 0: train loss 4.0001\n",
      "step 500: train loss 0.0009\n",
      "step 1000: train loss 0.0003\n",
      "step 1500: train loss 0.0001\n",
      "step 2000: train loss 0.0001\n",
      "step 2500: train loss 0.0001\n",
      "step 3000: train loss 0.0001\n",
      "step 3500: train loss 0.0001\n",
      "step 4000: train loss 0.0001\n",
      "step 4500: train loss 0.0001\n",
      "step 4999: train loss 0.0000\n"
     ]
    }
   ],
   "source": [
    "model_dummy = GPTLanguageModel()\n",
    "m_dummy = model_dummy.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m_dummy.parameters())/1e6, 'M parameters')\n",
    "\n",
    "data=train_dummy_tensor_encoded\n",
    "model=m_dummy\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model_dummy.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        # print(losses)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(data)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model_dummy(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b3ed395-292e-450b-b947-bc28363aff3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (0.00%) trials processed\n",
      "1000 (3.31%) trials processed\n",
      "2000 (6.61%) trials processed\n",
      "3000 (9.92%) trials processed\n",
      "4000 (13.23%) trials processed\n",
      "5000 (16.53%) trials processed\n",
      "6000 (19.84%) trials processed\n",
      "7000 (23.15%) trials processed\n",
      "8000 (26.46%) trials processed\n",
      "9000 (29.76%) trials processed\n",
      "10000 (33.07%) trials processed\n",
      "11000 (36.38%) trials processed\n",
      "12000 (39.68%) trials processed\n",
      "13000 (42.99%) trials processed\n",
      "14000 (46.30%) trials processed\n",
      "15000 (49.60%) trials processed\n",
      "16000 (52.91%) trials processed\n",
      "17000 (56.22%) trials processed\n",
      "18000 (59.52%) trials processed\n",
      "19000 (62.83%) trials processed\n",
      "20000 (66.14%) trials processed\n",
      "21000 (69.44%) trials processed\n",
      "22000 (72.75%) trials processed\n",
      "23000 (76.06%) trials processed\n",
      "24000 (79.37%) trials processed\n",
      "25000 (82.67%) trials processed\n",
      "26000 (85.98%) trials processed\n",
      "27000 (89.29%) trials processed\n",
      "28000 (92.59%) trials processed\n",
      "29000 (95.90%) trials processed\n",
      "30000 (99.21%) trials processed\n",
      "30239 (100.00%) trials processed\n"
     ]
    }
   ],
   "source": [
    "# ### Predict model data\n",
    "train_dummy_responses = get_trials_answers(train_dummy_tensor_encoded, m_dummy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
