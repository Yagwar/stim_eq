{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaf225de-a6c3-4c50-9f3b-3c99bc6b9059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Transformer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pickle\n",
    "\n",
    "## NanoGPS adds\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "587773bc-dcfe-4766-9c4a-e84a691f99fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n",
      "device cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA Available: \",torch.cuda.is_available())\n",
    "use_cuda=True\n",
    "device = torch.device(\"cuda:0\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd64a0c3-97b5-4286-ae65-6c8b19719da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"AB\",\"BC\",\"CD\",\"DE\",\"EF\",\"FG\"# LS \n",
    "# \"BA\",\"CA\",\"DA\",\"EA\",\"FA\",\"GA\"# MTO\n",
    "# \"AB\",\"AC\",\"AD\",\"AE\",\"AF\",\"AG\"# OTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e247f784-e495-400e-bf94-998e97e899aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainig pairs for the experiment ['AB', 'AC', 'AD', 'AE', 'AF', 'AG']\n",
      "Creating experiment with 7 members, 4 classes and 21 dummy stimulus.\n",
      "Experiment trials created!\n"
     ]
    }
   ],
   "source": [
    "%run ../utils/create_trials.py \"AB\",\"AC\",\"AD\",\"AE\",\"AF\",\"AG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e328d1f8-436f-42e7-bf73-e584acf9f3ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trials_info_columns=[\"st_sample\",\"st_comp1\",\"st_comp2\",\"st_comp3\",\"option_answer\"]#\"st_comparison\"\n",
    "\n",
    "train_trials_corpus_df=train_info[trials_info_columns]\n",
    "train_dummy_trials_corpus_df=train_dummy_info[trials_info_columns]#\"st_comparison\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9c9c376-5034-4f2b-a3ab-5da53222327d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_subset</th>\n",
       "      <th>st_sample</th>\n",
       "      <th>st_comp1</th>\n",
       "      <th>st_comp2</th>\n",
       "      <th>st_comp3</th>\n",
       "      <th>option_answer</th>\n",
       "      <th>st_comparison</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4655</th>\n",
       "      <td>train</td>\n",
       "      <td>A4</td>\n",
       "      <td>A2</td>\n",
       "      <td>G1</td>\n",
       "      <td>B4</td>\n",
       "      <td>O_3</td>\n",
       "      <td>B4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2331</th>\n",
       "      <td>train</td>\n",
       "      <td>A2</td>\n",
       "      <td>B2</td>\n",
       "      <td>F1</td>\n",
       "      <td>A1</td>\n",
       "      <td>O_1</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16563</th>\n",
       "      <td>train</td>\n",
       "      <td>A2</td>\n",
       "      <td>E2</td>\n",
       "      <td>D4</td>\n",
       "      <td>E4</td>\n",
       "      <td>O_1</td>\n",
       "      <td>E2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16050</th>\n",
       "      <td>train</td>\n",
       "      <td>A1</td>\n",
       "      <td>E1</td>\n",
       "      <td>A4</td>\n",
       "      <td>E3</td>\n",
       "      <td>O_1</td>\n",
       "      <td>E1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14348</th>\n",
       "      <td>train</td>\n",
       "      <td>A4</td>\n",
       "      <td>E3</td>\n",
       "      <td>G2</td>\n",
       "      <td>D4</td>\n",
       "      <td>O_3</td>\n",
       "      <td>D4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1830</th>\n",
       "      <td>train</td>\n",
       "      <td>A2</td>\n",
       "      <td>B2</td>\n",
       "      <td>G1</td>\n",
       "      <td>F4</td>\n",
       "      <td>O_1</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16810</th>\n",
       "      <td>train</td>\n",
       "      <td>A2</td>\n",
       "      <td>E3</td>\n",
       "      <td>E2</td>\n",
       "      <td>D4</td>\n",
       "      <td>O_2</td>\n",
       "      <td>E2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27079</th>\n",
       "      <td>train</td>\n",
       "      <td>A2</td>\n",
       "      <td>F3</td>\n",
       "      <td>G2</td>\n",
       "      <td>A3</td>\n",
       "      <td>O_2</td>\n",
       "      <td>G2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22966</th>\n",
       "      <td>train</td>\n",
       "      <td>A3</td>\n",
       "      <td>G2</td>\n",
       "      <td>F3</td>\n",
       "      <td>F1</td>\n",
       "      <td>O_2</td>\n",
       "      <td>F3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20971</th>\n",
       "      <td>train</td>\n",
       "      <td>A1</td>\n",
       "      <td>F3</td>\n",
       "      <td>F1</td>\n",
       "      <td>E3</td>\n",
       "      <td>O_2</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13882</th>\n",
       "      <td>train</td>\n",
       "      <td>A4</td>\n",
       "      <td>E1</td>\n",
       "      <td>D4</td>\n",
       "      <td>E3</td>\n",
       "      <td>O_2</td>\n",
       "      <td>D4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>train</td>\n",
       "      <td>A1</td>\n",
       "      <td>F4</td>\n",
       "      <td>D4</td>\n",
       "      <td>B1</td>\n",
       "      <td>O_3</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23987</th>\n",
       "      <td>train</td>\n",
       "      <td>A4</td>\n",
       "      <td>E1</td>\n",
       "      <td>F1</td>\n",
       "      <td>F4</td>\n",
       "      <td>O_3</td>\n",
       "      <td>F4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22939</th>\n",
       "      <td>train</td>\n",
       "      <td>A3</td>\n",
       "      <td>G2</td>\n",
       "      <td>F3</td>\n",
       "      <td>C2</td>\n",
       "      <td>O_2</td>\n",
       "      <td>F3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29912</th>\n",
       "      <td>train</td>\n",
       "      <td>A4</td>\n",
       "      <td>G3</td>\n",
       "      <td>D3</td>\n",
       "      <td>G4</td>\n",
       "      <td>O_3</td>\n",
       "      <td>G4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18806</th>\n",
       "      <td>train</td>\n",
       "      <td>A3</td>\n",
       "      <td>C1</td>\n",
       "      <td>E2</td>\n",
       "      <td>E3</td>\n",
       "      <td>O_3</td>\n",
       "      <td>E3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5023</th>\n",
       "      <td>train</td>\n",
       "      <td>A4</td>\n",
       "      <td>D1</td>\n",
       "      <td>B4</td>\n",
       "      <td>A2</td>\n",
       "      <td>O_2</td>\n",
       "      <td>B4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24652</th>\n",
       "      <td>train</td>\n",
       "      <td>A4</td>\n",
       "      <td>G1</td>\n",
       "      <td>F4</td>\n",
       "      <td>B2</td>\n",
       "      <td>O_2</td>\n",
       "      <td>F4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10313</th>\n",
       "      <td>train</td>\n",
       "      <td>A1</td>\n",
       "      <td>C3</td>\n",
       "      <td>B4</td>\n",
       "      <td>D1</td>\n",
       "      <td>O_3</td>\n",
       "      <td>D1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>train</td>\n",
       "      <td>A1</td>\n",
       "      <td>B4</td>\n",
       "      <td>B1</td>\n",
       "      <td>A3</td>\n",
       "      <td>O_2</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sample_subset st_sample st_comp1 st_comp2 st_comp3 option_answer  \\\n",
       "4655          train        A4       A2       G1       B4           O_3   \n",
       "2331          train        A2       B2       F1       A1           O_1   \n",
       "16563         train        A2       E2       D4       E4           O_1   \n",
       "16050         train        A1       E1       A4       E3           O_1   \n",
       "14348         train        A4       E3       G2       D4           O_3   \n",
       "1830          train        A2       B2       G1       F4           O_1   \n",
       "16810         train        A2       E3       E2       D4           O_2   \n",
       "27079         train        A2       F3       G2       A3           O_2   \n",
       "22966         train        A3       G2       F3       F1           O_2   \n",
       "20971         train        A1       F3       F1       E3           O_2   \n",
       "13882         train        A4       E1       D4       E3           O_2   \n",
       "848           train        A1       F4       D4       B1           O_3   \n",
       "23987         train        A4       E1       F1       F4           O_3   \n",
       "22939         train        A3       G2       F3       C2           O_2   \n",
       "29912         train        A4       G3       D3       G4           O_3   \n",
       "18806         train        A3       C1       E2       E3           O_3   \n",
       "5023          train        A4       D1       B4       A2           O_2   \n",
       "24652         train        A4       G1       F4       B2           O_2   \n",
       "10313         train        A1       C3       B4       D1           O_3   \n",
       "1105          train        A1       B4       B1       A3           O_2   \n",
       "\n",
       "      st_comparison  \n",
       "4655             B4  \n",
       "2331             B2  \n",
       "16563            E2  \n",
       "16050            E1  \n",
       "14348            D4  \n",
       "1830             B2  \n",
       "16810            E2  \n",
       "27079            G2  \n",
       "22966            F3  \n",
       "20971            F1  \n",
       "13882            D4  \n",
       "848              B1  \n",
       "23987            F4  \n",
       "22939            F3  \n",
       "29912            G4  \n",
       "18806            E3  \n",
       "5023             B4  \n",
       "24652            F4  \n",
       "10313            D1  \n",
       "1105             B1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_info.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28a3350e-ac19-4fd6-b4ec-9843fe0a7fa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokens_list=list(stims_random.keys())\n",
    "tokens_list=sorted(list(set((list(np.array(train_trials_corpus_df).reshape([1,-1])[0])+(list(np.array(train_dummy_trials_corpus_df).reshape([1,-1])[0]))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e007479-c50e-4012-98d1-95a2f280210b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_to_index = {token: index for index, token in enumerate(tokens_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48ecc37c-b7fa-46a7-bf8b-691b676688d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode corpus in positional tokens train data Dummy comparison\n",
    "encoded_training_data = np.array([[token_to_index[token] for token in sequence] for sequence in np.array(train_trials_corpus_df)])\n",
    "train_tensor_encoded=torch.from_numpy(encoded_training_data).int().to(device)\n",
    "train_tensor_encoded=train_tensor_encoded.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80319013-bb32-4b78-9cfe-34974ad0621a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encode corpus in positional tokens Train data class member comparison\n",
    "encoded_training_data_dummy = np.array([[token_to_index[token] for token in sequence] for sequence in np.array(train_dummy_trials_corpus_df)])\n",
    "train_dummy_tensor_encoded=torch.from_numpy(encoded_training_data_dummy).int().to(device)\n",
    "train_dummy_tensor_encoded=train_dummy_tensor_encoded.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c441e51-6f27-4146-a754-7b43ade70d37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set seed for reproductibility\n",
    "torch.manual_seed(183)\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 4 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 200\n",
    "\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "341f5a0c-0e84-438a-916c-d45b791125db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = tokens_list\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: [itos[i] for i in l]#lambda l: ','.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "122c6bbc-01a3-4df5-83c3-3761438b4b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(data):\n",
    "    # data must be train trials encoded and transformed in torch tensor\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    # data = train_tensor_encoded #if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data), (batch_size,)) \n",
    "    x = torch.stack([data[i,:block_size] for i in ix])\n",
    "    y = torch.stack([data[i,1:] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    # data = train_tensor_encoded# Modify for train several models\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        X, Y = get_batch(data)\n",
    "        logits, loss = model(X, Y)\n",
    "        losses[k] = loss.item()\n",
    "    out[\"train\"] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "584160b4-7b06-4dde-9016-b77fdd99918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        # wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T) #filter communication with past# comment this and you have an encoder block\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "283a6d2d-132e-47e5-b483-60e498be6981",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_trials_answers(tensor_encoded, model_trained, tell_me=True, tell_me_n_trials=1000):\n",
    "    # tensor_encoded: Pytorch tensor long(float64) type [sample, comp_1, comp_2, comp_3, comp_response] of all group trials\n",
    "    max_iters=tensor_encoded.shape[0]\n",
    "    response_stimulus=[]\n",
    "    for count, trial in enumerate(tensor_encoded[:,:-1]):\n",
    "        tokens_response=model_trained.generate(trial.reshape([1,-1]), max_new_tokens=1)[0].tolist()\n",
    "        response=decode(tokens_response)[-1]\n",
    "        response_stimulus.append(response)\n",
    "\n",
    "        if tell_me:\n",
    "            # tell me the progress every tell_me_n_trials\n",
    "            if count % tell_me_n_trials == 0 or count == max_iters - 1:\n",
    "                print(f\"{count} ({100.0*(count/max_iters):.2f}%) trials processed\")\n",
    "    return response_stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e3ec339-32f2-4fe7-8a78-663b90fff760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########### Model_1 train trials class comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48a1e775-cc9a-4fcb-bf30-184bbdec56bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.682164 M parameters\n",
      "step 0: train loss 4.0410\n",
      "step 500: train loss 0.0011\n",
      "step 1000: train loss 0.0004\n",
      "step 1500: train loss 0.0001\n",
      "step 2000: train loss 0.0001\n",
      "step 2500: train loss 0.0001\n",
      "step 3000: train loss 0.0002\n",
      "step 3500: train loss 0.0002\n",
      "step 4000: train loss 0.0000\n",
      "step 4500: train loss 0.0000\n",
      "step 4999: train loss 0.0000\n"
     ]
    }
   ],
   "source": [
    "model_class = GPTLanguageModel()\n",
    "m_class = model_class.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m_class.parameters())/1e6, 'M parameters')\n",
    "\n",
    "data=train_tensor_encoded\n",
    "model=m_class\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model_class.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        # print(losses)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(data)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model_class(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44a0302b-9e35-4d48-8a04-49c60308f2e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Predict model data\n",
    "\n",
    "# train_responses=get_trials_answers(train_tensor_encoded, m_class)\n",
    "# train_responses_info=train_info.copy()\n",
    "# train_responses_info[\"response\"]=train_responses\n",
    "# train_responses_info[\"response_val\"]=(train_responses_info.st_comparison==train_responses_info.response)*1\n",
    "\n",
    "# sum(train_responses_info.response_val)/train_responses_info.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f0fcae8-46ad-4c1b-a938-7d491438718f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########### Model_2 train trials dummy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3851ad4-7c59-413d-ac81-754b46fb029a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.682164 M parameters\n",
      "step 0: train loss 4.0001\n",
      "step 500: train loss 0.0012\n",
      "step 1000: train loss 0.0004\n",
      "step 1500: train loss 0.0002\n",
      "step 2000: train loss 0.0001\n",
      "step 2500: train loss 0.0001\n",
      "step 3000: train loss 0.0001\n",
      "step 3500: train loss 0.0000\n",
      "step 4000: train loss 0.0002\n",
      "step 4500: train loss 0.0000\n",
      "step 4999: train loss 0.0000\n"
     ]
    }
   ],
   "source": [
    "model_dummy = GPTLanguageModel()\n",
    "m_dummy = model_dummy.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m_dummy.parameters())/1e6, 'M parameters')\n",
    "\n",
    "data=train_dummy_tensor_encoded\n",
    "model=m_dummy\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model_dummy.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        # print(losses)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(data)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model_dummy(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b3ed395-292e-450b-b947-bc28363aff3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### Predict model data\n",
    "\n",
    "# train_dummy_responses=get_trials_answers(train_dummy_tensor_encoded, m_dummy)\n",
    "\n",
    "# train_dummy_responses_info=train_dummy_info.copy()\n",
    "# train_dummy_responses_info[\"response\"]=train_dummy_responses\n",
    "# train_dummy_responses_info[\"response_val\"]=(train_dummy_responses_info.st_comparison==train_dummy_responses_info.response)*1\n",
    "\n",
    "# sum(train_dummy_responses_info.response_val)/train_dummy_responses_info.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdf3f5a3-1806-4625-be55-402816fb66e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Model_3 reflexivity trials dummy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea67347e-95cd-42e6-b67d-6f2d888b335e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reflexivity_dummy_trials_corpus_df=reflexivity_dummy_info[trials_info_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d17c85d-6c08-405f-97e9-d0167cefffe8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encode corpus in positional tokens\n",
    "encoded_reflexivity_dummy_data = np.array([[token_to_index[token] for token in sequence] for sequence in np.array(reflexivity_dummy_trials_corpus_df)])\n",
    "reflexivity_dummy_tensor_encoded=torch.from_numpy(encoded_reflexivity_dummy_data).int().to(device)\n",
    "reflexivity_dummy_tensor_encoded=reflexivity_dummy_tensor_encoded.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c8a2553-679d-4c62-8681-009737915b15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.682164 M parameters\n",
      "step 0: train loss 3.9744\n",
      "step 500: train loss 0.0011\n",
      "step 1000: train loss 0.0006\n",
      "step 1500: train loss 0.0003\n",
      "step 2000: train loss 0.0002\n",
      "step 2500: train loss 0.0001\n",
      "step 3000: train loss 0.0001\n",
      "step 3500: train loss 0.0002\n",
      "step 4000: train loss 0.0001\n",
      "step 4500: train loss 0.0001\n",
      "step 4999: train loss 0.0001\n"
     ]
    }
   ],
   "source": [
    "model_reflexivity_dummy = GPTLanguageModel()\n",
    "m_reflexivity_dummy = model_reflexivity_dummy.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m_reflexivity_dummy.parameters())/1e6, 'M parameters')\n",
    "\n",
    "data=reflexivity_dummy_tensor_encoded\n",
    "model=m_reflexivity_dummy\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model_reflexivity_dummy.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        # print(losses)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(data)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m_reflexivity_dummy(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58da159c-56fc-4343-a91b-948a98f9a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Predict model data\n",
    "# reflexivity_dummy_responses=get_trials_answers(reflexivity_dummy_tensor_encoded, m_reflexivity_dummy)\n",
    "\n",
    "# reflexivity_dummy_responses_info=reflexivity_dummy_info.copy()\n",
    "# reflexivity_dummy_responses_info[\"response\"]=reflexivity_dummy_responses\n",
    "# reflexivity_dummy_responses_info[\"response_val\"]=(reflexivity_dummy_responses_info.st_comparison==reflexivity_dummy_responses_info.response)*1\n",
    "\n",
    "# sum(reflexivity_dummy_responses_info.response_val)/reflexivity_dummy_responses_info.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6669d7b3-4e9d-43eb-810e-113fbcd5f297",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Model_4 reflexivity trials class members comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebc71836-faa2-42b7-b09c-ce5c91c88f44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reflexivity_trials_corpus_df=reflexivity_info[trials_info_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "074ae862-7890-41c4-9522-88e4626f6ae7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encode corpus in positional tokens\n",
    "encoded_reflexivity_data = np.array([[token_to_index[token] for token in sequence] for sequence in np.array(reflexivity_trials_corpus_df)])\n",
    "reflexivity_tensor_encoded=torch.from_numpy(encoded_reflexivity_data).int().to(device)\n",
    "reflexivity_tensor_encoded=reflexivity_tensor_encoded.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b170699-da13-4186-996f-73b874cb6354",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.682164 M parameters\n",
      "step 0: train loss 4.0132\n",
      "step 500: train loss 0.0011\n",
      "step 1000: train loss 0.0002\n",
      "step 1500: train loss 0.0005\n",
      "step 2000: train loss 0.0003\n",
      "step 2500: train loss 0.0000\n",
      "step 3000: train loss 0.0000\n",
      "step 3500: train loss 0.0001\n",
      "step 4000: train loss 0.0000\n",
      "step 4500: train loss 0.0000\n",
      "step 4999: train loss 0.0000\n"
     ]
    }
   ],
   "source": [
    "model_reflexivity = GPTLanguageModel()\n",
    "m_reflexivity = model_reflexivity.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m_reflexivity.parameters())/1e6, 'M parameters')\n",
    "\n",
    "data=reflexivity_tensor_encoded\n",
    "model=m_reflexivity\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model_reflexivity.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        # print(losses)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(data)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m_reflexivity(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83367e55-538b-4228-9ec5-82aa9ec40d8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Predict model data\n",
    "\n",
    "# reflexivity_responses=get_trials_answers(reflexivity_tensor_encoded, m_reflexivity)\n",
    "\n",
    "# reflexivity_responses_info=reflexivity_info.copy()\n",
    "# reflexivity_responses_info[\"response\"]=reflexivity_responses\n",
    "# reflexivity_responses_info[\"response_val\"]=(reflexivity_responses_info.st_comparison==reflexivity_responses_info.response)*1\n",
    "\n",
    "# sum(reflexivity_responses_info.response_val)/reflexivity_responses_info.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "becb268d-e85b-470a-afb7-1c8a6aed0c12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Persist_ models\n",
    "# models_dict={\n",
    "#     'm_class':m_class,\n",
    "#     'm_dummy':m_dummy,\n",
    "#     'm_reflexivity':m_reflexivity,\n",
    "#     'm_reflexivity_dummy':m_reflexivity_dummy\n",
    "# }\n",
    "\n",
    "# with open('salidas/SE_NAnoGPT_4models_option.pickle', 'wb') as f:\n",
    "#     pickle.dump(models_dict, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aefa281a-5d99-4c23-bb9b-12caa6d7798b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### Load trainded models\n",
    "\n",
    "# with open('salidas/SE_NAnoGPT_4models.pickle', 'rb') as f:\n",
    "#     models_dict = pickle.load(f)\n",
    "\n",
    "# m_class = models_dict['m_class'].to(device)\n",
    "# m_dummy = models_dict['m_dummy'].to(device)\n",
    "# m_reflexivity = models_dict['m_reflexivity'].to(device)\n",
    "# m_reflexivity_dummy = models_dict['m_reflexivity_dummy'].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bad8ec43-083f-4c80-be4d-f9c0641031f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########## ALL trials 4 models responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a1ddf1c-bc73-4242-87fd-5104ee3d83f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_trials=train_info.copy()\n",
    "train_dummy_trials=train_dummy_info.copy()\n",
    "reflexivity_trials=reflexivity_info.copy()\n",
    "reflexivity_dummy_trials=reflexivity_dummy_info.copy()\n",
    "symmetry_trials=symmetry_info.copy()\n",
    "transitivity_trials=transitivity_info.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be8747fc-e571-4e76-b4cf-65e05270f5bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dummy_trials.sample_subset=\"train_dummy\"\n",
    "reflexivity_dummy_trials.sample_subset=\"reflexivity_dummy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2cd9246e-98aa-4b93-8450-684104c3ec43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_trials=pd.concat([\n",
    "    train_trials,\n",
    "    train_dummy_trials,\n",
    "    reflexivity_trials,\n",
    "    reflexivity_dummy_trials,\n",
    "    symmetry_trials,\n",
    "    transitivity_trials\n",
    "],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25b258e5-8f73-4aa8-bdab-4d1ed27dd54e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trials_corpus_df=full_trials[trials_info_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d513ae13-c7f6-4fb8-ad8e-738479877f1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encode corpus in positional tokens\n",
    "encoded_trials_data = np.array([[token_to_index[token] for token in sequence] for sequence in np.array(trials_corpus_df)])\n",
    "trials_tensor_encoded=torch.from_numpy(encoded_trials_data).int().to(device)\n",
    "trials_tensor_encoded=trials_tensor_encoded.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5712f00a-48de-4436-b52e-a9f95fc1971b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_trials_report=int(trials_tensor_encoded.shape[0]/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5d629fe-71fc-48c6-9550-71a43a088333",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1\n",
      "0 (0.00%) trials processed\n",
      "31248 (10.00%) trials processed\n",
      "62496 (20.00%) trials processed\n",
      "93744 (30.00%) trials processed\n",
      "124992 (40.00%) trials processed\n",
      "156240 (50.00%) trials processed\n",
      "187488 (60.00%) trials processed\n",
      "218736 (70.00%) trials processed\n",
      "249984 (80.00%) trials processed\n",
      "281232 (90.00%) trials processed\n",
      "312479 (100.00%) trials processed\n",
      "model 2\n",
      "0 (0.00%) trials processed\n",
      "31248 (10.00%) trials processed\n",
      "62496 (20.00%) trials processed\n",
      "93744 (30.00%) trials processed\n",
      "124992 (40.00%) trials processed\n",
      "156240 (50.00%) trials processed\n",
      "187488 (60.00%) trials processed\n",
      "218736 (70.00%) trials processed\n",
      "249984 (80.00%) trials processed\n",
      "281232 (90.00%) trials processed\n",
      "312479 (100.00%) trials processed\n",
      "model 3\n",
      "0 (0.00%) trials processed\n",
      "31248 (10.00%) trials processed\n",
      "62496 (20.00%) trials processed\n",
      "93744 (30.00%) trials processed\n",
      "124992 (40.00%) trials processed\n",
      "156240 (50.00%) trials processed\n",
      "187488 (60.00%) trials processed\n",
      "218736 (70.00%) trials processed\n",
      "249984 (80.00%) trials processed\n",
      "281232 (90.00%) trials processed\n",
      "312479 (100.00%) trials processed\n",
      "model 4\n",
      "0 (0.00%) trials processed\n",
      "31248 (10.00%) trials processed\n",
      "62496 (20.00%) trials processed\n",
      "93744 (30.00%) trials processed\n",
      "124992 (40.00%) trials processed\n",
      "156240 (50.00%) trials processed\n",
      "187488 (60.00%) trials processed\n",
      "218736 (70.00%) trials processed\n",
      "249984 (80.00%) trials processed\n",
      "281232 (90.00%) trials processed\n",
      "312479 (100.00%) trials processed\n"
     ]
    }
   ],
   "source": [
    "print(\"model 1\")\n",
    "trials_responses_m_class=get_trials_answers(trials_tensor_encoded, m_class, tell_me_n_trials=n_trials_report)\n",
    "print(\"model 2\")\n",
    "trials_responses_m_dummy=get_trials_answers(trials_tensor_encoded, m_dummy, tell_me_n_trials=n_trials_report)\n",
    "print(\"model 3\")\n",
    "trials_responses_m_reflexivity=get_trials_answers(trials_tensor_encoded, m_reflexivity, tell_me_n_trials=n_trials_report)\n",
    "print(\"model 4\")\n",
    "trials_responses_m_reflexivity_dummy=get_trials_answers(trials_tensor_encoded, m_reflexivity_dummy, tell_me_n_trials=n_trials_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "772e0704-b12d-4c51-ad3f-96482e5074c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_trials[\"response_m_class\"]=trials_responses_m_class\n",
    "full_trials[\"response_m_dummy\"]=trials_responses_m_dummy\n",
    "full_trials[\"response_m_reflexivity\"]=trials_responses_m_reflexivity\n",
    "full_trials[\"response_m_reflexivity_dummy\"]=trials_responses_m_reflexivity_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "684b647a-d04a-4c41-bb9a-ef906dcb1a57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_subset</th>\n",
       "      <th>st_sample</th>\n",
       "      <th>st_comp1</th>\n",
       "      <th>st_comp2</th>\n",
       "      <th>st_comp3</th>\n",
       "      <th>option_answer</th>\n",
       "      <th>st_comparison</th>\n",
       "      <th>response_m_class</th>\n",
       "      <th>response_m_dummy</th>\n",
       "      <th>response_m_reflexivity</th>\n",
       "      <th>response_m_reflexivity_dummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>E4</td>\n",
       "      <td>C4</td>\n",
       "      <td>O_1</td>\n",
       "      <td>B1</td>\n",
       "      <td>O_1</td>\n",
       "      <td>O_3</td>\n",
       "      <td>O_1</td>\n",
       "      <td>O_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>A1</td>\n",
       "      <td>E4</td>\n",
       "      <td>B1</td>\n",
       "      <td>C4</td>\n",
       "      <td>O_2</td>\n",
       "      <td>B1</td>\n",
       "      <td>O_2</td>\n",
       "      <td>O_3</td>\n",
       "      <td>O_2</td>\n",
       "      <td>O_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>A1</td>\n",
       "      <td>E4</td>\n",
       "      <td>C4</td>\n",
       "      <td>B1</td>\n",
       "      <td>O_3</td>\n",
       "      <td>B1</td>\n",
       "      <td>O_3</td>\n",
       "      <td>O_3</td>\n",
       "      <td>O_3</td>\n",
       "      <td>O_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>E4</td>\n",
       "      <td>D4</td>\n",
       "      <td>O_1</td>\n",
       "      <td>B1</td>\n",
       "      <td>O_1</td>\n",
       "      <td>O_1</td>\n",
       "      <td>O_1</td>\n",
       "      <td>O_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>A1</td>\n",
       "      <td>E4</td>\n",
       "      <td>B1</td>\n",
       "      <td>D4</td>\n",
       "      <td>O_2</td>\n",
       "      <td>B1</td>\n",
       "      <td>O_2</td>\n",
       "      <td>O_3</td>\n",
       "      <td>O_2</td>\n",
       "      <td>O_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312475</th>\n",
       "      <td>transitivity</td>\n",
       "      <td>G4</td>\n",
       "      <td>D1</td>\n",
       "      <td>F4</td>\n",
       "      <td>B2</td>\n",
       "      <td>O_2</td>\n",
       "      <td>F4</td>\n",
       "      <td>O_1</td>\n",
       "      <td>O_3</td>\n",
       "      <td>O_2</td>\n",
       "      <td>O_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312476</th>\n",
       "      <td>transitivity</td>\n",
       "      <td>G4</td>\n",
       "      <td>D1</td>\n",
       "      <td>B2</td>\n",
       "      <td>F4</td>\n",
       "      <td>O_3</td>\n",
       "      <td>F4</td>\n",
       "      <td>O_3</td>\n",
       "      <td>O_3</td>\n",
       "      <td>O_3</td>\n",
       "      <td>O_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312477</th>\n",
       "      <td>transitivity</td>\n",
       "      <td>G4</td>\n",
       "      <td>F4</td>\n",
       "      <td>D1</td>\n",
       "      <td>C1</td>\n",
       "      <td>O_1</td>\n",
       "      <td>F4</td>\n",
       "      <td>O_1</td>\n",
       "      <td>O_1</td>\n",
       "      <td>O_1</td>\n",
       "      <td>O_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312478</th>\n",
       "      <td>transitivity</td>\n",
       "      <td>G4</td>\n",
       "      <td>D1</td>\n",
       "      <td>F4</td>\n",
       "      <td>C1</td>\n",
       "      <td>O_2</td>\n",
       "      <td>F4</td>\n",
       "      <td>O_2</td>\n",
       "      <td>O_3</td>\n",
       "      <td>O_2</td>\n",
       "      <td>O_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312479</th>\n",
       "      <td>transitivity</td>\n",
       "      <td>G4</td>\n",
       "      <td>D1</td>\n",
       "      <td>C1</td>\n",
       "      <td>F4</td>\n",
       "      <td>O_3</td>\n",
       "      <td>F4</td>\n",
       "      <td>O_3</td>\n",
       "      <td>O_3</td>\n",
       "      <td>O_3</td>\n",
       "      <td>O_1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>312480 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample_subset st_sample st_comp1 st_comp2 st_comp3 option_answer  \\\n",
       "0              train        A1       B1       E4       C4           O_1   \n",
       "1              train        A1       E4       B1       C4           O_2   \n",
       "2              train        A1       E4       C4       B1           O_3   \n",
       "3              train        A1       B1       E4       D4           O_1   \n",
       "4              train        A1       E4       B1       D4           O_2   \n",
       "...              ...       ...      ...      ...      ...           ...   \n",
       "312475  transitivity        G4       D1       F4       B2           O_2   \n",
       "312476  transitivity        G4       D1       B2       F4           O_3   \n",
       "312477  transitivity        G4       F4       D1       C1           O_1   \n",
       "312478  transitivity        G4       D1       F4       C1           O_2   \n",
       "312479  transitivity        G4       D1       C1       F4           O_3   \n",
       "\n",
       "       st_comparison response_m_class response_m_dummy response_m_reflexivity  \\\n",
       "0                 B1              O_1              O_3                    O_1   \n",
       "1                 B1              O_2              O_3                    O_2   \n",
       "2                 B1              O_3              O_3                    O_3   \n",
       "3                 B1              O_1              O_1                    O_1   \n",
       "4                 B1              O_2              O_3                    O_2   \n",
       "...              ...              ...              ...                    ...   \n",
       "312475            F4              O_1              O_3                    O_2   \n",
       "312476            F4              O_3              O_3                    O_3   \n",
       "312477            F4              O_1              O_1                    O_1   \n",
       "312478            F4              O_2              O_3                    O_2   \n",
       "312479            F4              O_3              O_3                    O_3   \n",
       "\n",
       "       response_m_reflexivity_dummy  \n",
       "0                               O_1  \n",
       "1                               O_3  \n",
       "2                               O_3  \n",
       "3                               O_3  \n",
       "4                               O_3  \n",
       "...                             ...  \n",
       "312475                          O_3  \n",
       "312476                          O_3  \n",
       "312477                          O_1  \n",
       "312478                          O_3  \n",
       "312479                          O_1  \n",
       "\n",
       "[312480 rows x 11 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e13d350-cc97-4099-b482-0c4b4cdbf25d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Export Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a4716ab8-cc65-4bdd-a431-0222a7c29f68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('salidas/encoder_trials_responses_option_OTM.pickle', 'wb') as f:\n",
    "    pickle.dump(full_trials, f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
